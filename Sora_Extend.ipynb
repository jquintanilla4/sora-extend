{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshumer/sora-extend/blob/main/Sora_Extend.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_OryDkwDDu9"
      },
      "source": [
        "# Sora 2 — AI‑Planned, Scene‑Exact Prompts with Continuity (Chained >12s)\n",
        "\n",
        "Built originally by [Matt Shumer](https://x.com/mattshumer_).\n",
        "\n",
        "Pipeline:\n",
        "1) Use an LLM (“GPT‑5 Thinking”) to plan N scene prompts from a base idea. The LLM is prompted to do this intelligently to enable continuity.\n",
        "2) Render each segment with Sora 2; for continuity, pass the prior segment’s **final frame** as `input_reference`.\n",
        "3) Concatenate segments into a single MP4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "EiRiFUsnR3A-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment: Local\n",
            "Checking dependencies...\n",
            "⚠ MoviePy installation failed, falling back to ffmpeg\n",
            "✓ FFmpeg available at: /Users/jquintanilla/Developer/sora-extend/.venv/lib/python3.11/site-packages/imageio_ffmpeg/binaries/ffmpeg-macos-aarch64-v7.1\n",
            "✓ Video processing backend: FFmpeg\n",
            "Installing additional packages...\n",
            "✓ All imports successful\n"
          ]
        }
      ],
      "source": [
        "# @title 1) Install & imports\n",
        "\n",
        "import sys, subprocess, importlib.util, shutil, os, textwrap, tempfile\n",
        "\n",
        "# Detect environment for informational purposes\n",
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "IN_COLAB = is_colab()\n",
        "print(f\"Environment: {'Colab' if IN_COLAB else 'Local'}\")\n",
        "\n",
        "def pip_install(*pkgs):\n",
        "    \"\"\"Install packages using uv pip\"\"\"\n",
        "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"-q\", \"-U\", *pkgs])\n",
        "\n",
        "def ensure(spec_name, *pip_pkgs):\n",
        "    \"\"\"Check if a package is available, install if needed\"\"\"\n",
        "    if importlib.util.find_spec(spec_name) is None:\n",
        "        print(f\"Installing {spec_name}...\")\n",
        "        pip_install(*pip_pkgs)\n",
        "    return importlib.util.find_spec(spec_name) is not None\n",
        "\n",
        "# Check/install core dependencies\n",
        "print(\"Checking dependencies...\")\n",
        "MOVIEPY_AVAILABLE = ensure(\"moviepy\", \"moviepy>=2.0.0\", \"imageio\", \"imageio-ffmpeg\")\n",
        "\n",
        "# Only import MoviePy if it's actually available\n",
        "if MOVIEPY_AVAILABLE:\n",
        "    try:\n",
        "        from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
        "        print(\"✓ MoviePy imported successfully\")\n",
        "    except ImportError:\n",
        "        MOVIEPY_AVAILABLE = False\n",
        "        print(\"⚠ MoviePy installation failed, falling back to ffmpeg\")\n",
        "\n",
        "# Fallback: ensure ffmpeg is available if MoviePy isn't\n",
        "if not MOVIEPY_AVAILABLE:\n",
        "    try:\n",
        "        import imageio_ffmpeg\n",
        "        FFMPEG_BIN = imageio_ffmpeg.get_ffmpeg_exe()\n",
        "    except Exception:\n",
        "        FFMPEG_BIN = shutil.which(\"ffmpeg\")\n",
        "\n",
        "    if not FFMPEG_BIN:\n",
        "        # Final attempt to get ffmpeg via pip\n",
        "        print(\"Installing imageio-ffmpeg...\")\n",
        "        pip_install(\"imageio-ffmpeg\")\n",
        "        try:\n",
        "            import imageio_ffmpeg\n",
        "            FFMPEG_BIN = imageio_ffmpeg.get_ffmpeg_exe()\n",
        "        except Exception:\n",
        "            FFMPEG_BIN = None\n",
        "\n",
        "    if not FFMPEG_BIN:\n",
        "        raise RuntimeError(\n",
        "            \"FFmpeg not found and MoviePy unavailable. \"\n",
        "            \"Install ffmpeg on your system or allow pip installs.\"\n",
        "        )\n",
        "    print(f\"✓ FFmpeg available at: {FFMPEG_BIN}\")\n",
        "\n",
        "print(f\"✓ Video processing backend: {'MoviePy' if MOVIEPY_AVAILABLE else 'FFmpeg'}\")\n",
        "\n",
        "# Install remaining dependencies\n",
        "print(\"Installing additional packages...\")\n",
        "if IN_COLAB:\n",
        "    get_ipython().system('uv pip -q install --upgrade openai requests opencv-python-headless imageio[ffmpeg] fal-client')\n",
        "else:\n",
        "    # Use pip_install for consistency in local environments too\n",
        "    pip_install(\"openai\", \"requests\", \"opencv-python-headless\", \"imageio[ffmpeg]\", \"fal-client\")\n",
        "\n",
        "# Standard imports\n",
        "import os, re, io, json, time, math, mimetypes\n",
        "from pathlib import Path\n",
        "import requests\n",
        "import cv2\n",
        "from IPython.display import Video as IPyVideo, display\n",
        "from openai import OpenAI\n",
        "\n",
        "print(\"✓ All imports successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hpt3gFMtDMzO"
      },
      "source": [
        "# 2) Config\n",
        "\n",
        "Fill in your `.env` file with:\n",
        "- `OPENROUTER_API_KEY` - Your OpenRouter API key\n",
        "- `FAL_KEY` - Your Fal AI API key\n",
        "- `PLANNER_MODEL` - If you have access to \"GPT-5 Thinking\", set it below. Otherwise, fallback to a strong reasoning model you have.\n",
        "\n",
        "Configure:\n",
        "- `RESOLUTION` - Video resolution: `\"720p\"` or `\"1080p\"` (default: `\"720p\"`)\n",
        "- `ASPECT_RATIO` - Video aspect ratio: `\"16:9\"` or `\"9:16\"` (default: `\"16:9\"`)\n",
        "- `SECONDS_PER_SEGMENT` - Duration per segment: `4`, `8`, or `12` seconds (default: `8`)\n",
        "- `NUM_GENERATIONS` - Total number of segments to generate (default: `2`)\n",
        "\n",
        "**Continuity Logic:**\n",
        "- First segment uses `text-to-video` endpoint (no prior frame)\n",
        "- Subsequent segments use `image-to-video` endpoint (with last frame from previous segment)\n",
        "\n",
        "Total video length = `SECONDS_PER_SEGMENT * NUM_GENERATIONS`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NO-yBwL-DLjY"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "openrouter_key = os.getenv(\"OPENROUTER_KEY\")\n",
        "fal_key = os.getenv(\"FAL_KEY\")\n",
        "\n",
        "# Set up OpenRouter client (uses OpenAI SDK with custom base_url)\n",
        "client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=openrouter_key,\n",
        ")\n",
        "\n",
        "# OpenRouter model - GPT-5 with reasoning enabled\n",
        "PLANNER_MODEL = os.environ.get(\"PLANNER_MODEL\", \"openai/gpt-5\")\n",
        "\n",
        "# Set Fal AI key as environment variable for fal_client\n",
        "os.environ[\"FAL_KEY\"] = fal_key\n",
        "\n",
        "# Sora 2 endpoints (dynamically selected based on whether input image exists)\n",
        "SORA_TEXT_TO_VIDEO = \"fal-ai/sora-2/text-to-video/pro\"\n",
        "SORA_IMAGE_TO_VIDEO = \"fal-ai/sora-2/image-to-video/pro\"\n",
        "\n",
        "# Video settings for Fal AI Sora API\n",
        "RESOLUTION = \"720p\"      # Options: \"720p\", \"1080p\"\n",
        "ASPECT_RATIO = \"16:9\"    # Options: \"16:9\", \"9:16\"\n",
        "\n",
        "BASE_PROMPT          = \"Gameplay footage of a game releasing in 2027, a car driving through a futuristic city\"\n",
        "SECONDS_PER_SEGMENT  = 8  # Options: 4, 8, 12 (per Fal API duration enum)\n",
        "NUM_GENERATIONS      = 2\n",
        "\n",
        "# Output directory\n",
        "OUT_DIR = Path(\"sora_output\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Progress display\n",
        "PRINT_PROGRESS_BAR = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCtddkS2TwG2"
      },
      "source": [
        "# 3) The planner system prompt\n",
        "\n",
        "We’ll ask the planner model to output a clean JSON object with one prompt per generation.\n",
        "The prompts contain context and the actual shot details, maximizing continuity.\n",
        "\n",
        "This isn't super optimized and was a first pass done by GPT. If people like this notebook, let me know on X, and I'll improve this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Q4VI67aDaJH"
      },
      "outputs": [],
      "source": [
        "PLANNER_SYSTEM_INSTRUCTIONS = r\"\"\"\n",
        "You are a senior prompt director for Sora 2. Your job is to transform:\n",
        "- a Base prompt (broad idea),\n",
        "- a fixed generation length per segment (seconds),\n",
        "- and a total number of generations (N),\n",
        "\n",
        "into **N crystal-clear shot prompts** with **maximum continuity** across segments.\n",
        "\n",
        "Rules:\n",
        "1) Return **valid JSON** only. Structure:\n",
        "   {\n",
        "     \"segments\": [\n",
        "       {\n",
        "         \"title\": \"Generation 1\",\n",
        "         \"seconds\": 6,\n",
        "         \"prompt\": \"<prompt block to send into Sora>\"\n",
        "       },\n",
        "       ...\n",
        "     ]\n",
        "   }\n",
        "   - `seconds` MUST equal the given generation length for ALL segments.\n",
        "   - `prompt` should include a **Context** section for model guidance AND a **Prompt** line for the shot itself,\n",
        "     exactly like in the example below.\n",
        "2) Continuity:\n",
        "   - Segment 1 starts fresh from the BASE PROMPT.\n",
        "   - Segment k (k>1) must **begin exactly at the final frame** of segment k-1.\n",
        "   - Maintain consistent visual style, tone, lighting, and subject identity unless explicitly told to change.\n",
        "3) Safety & platform constraints:\n",
        "   - Do not depict real people (including public figures) or copyrighted characters.\n",
        "   - Avoid copyrighted music and avoid exact trademark/logos if policy disallows them; use brand-safe wording.\n",
        "   - Keep content suitable for general audiences.\n",
        "4) Output only JSON (no Markdown, no backticks).\n",
        "5) Keep the **Context** lines inside the prompt text (they're for the AI, not visible).\n",
        "6) Make the writing specific and cinematic; describe camera, lighting, motion, and subject focus succinctly.\n",
        "\n",
        "Below is an **EXAMPLE (verbatim)** of exactly how to structure prompts with context and continuity:\n",
        "\n",
        "Example:\n",
        "Base prompt: \"Intro video for the iPhone 19\"\n",
        "Generation length: 6 seconds each\n",
        "Total generations: 3\n",
        "\n",
        "Clearly defined prompts with maximum continuity and context:\n",
        "\n",
        "### Generation 1:\n",
        "\n",
        "<prompt>\n",
        "First shot introducing the new iPhone 19. Initially, the screen is completely dark. The phone, positioned vertically and facing directly forward, emerges slowly and dramatically out of darkness, gradually illuminated from the center of the screen outward, showcasing a vibrant, colorful, dynamic wallpaper on its edge-to-edge glass display. The style is futuristic, sleek, and premium, appropriate for an official Apple product reveal.\n",
        "<prompt>\n",
        "\n",
        "---\n",
        "\n",
        "### Generation 2:\n",
        "\n",
        "<prompt>\n",
        "Context (not visible in video, only for AI guidance):\n",
        "\n",
        "* You are creating the second part of an official intro video for Apple's new iPhone 19.\n",
        "* The previous 6-second scene ended with the phone facing directly forward, clearly displaying its vibrant front screen and colorful wallpaper.\n",
        "\n",
        "Prompt: Second shot begins exactly from the final frame of the previous scene, showing the front of the iPhone 19 with its vibrant, colorful display clearly visible. Now, smoothly rotate the phone horizontally, turning it from the front to reveal the back side. Focus specifically on the advanced triple-lens camera module, clearly highlighting its premium materials, reflective metallic surfaces, and detailed lenses. Maintain consistent dramatic lighting, sleek visual style, and luxurious feel matching the official Apple product introduction theme.\n",
        "</prompt>\n",
        "\n",
        "---\n",
        "\n",
        "### Generation 3:\n",
        "\n",
        "<prompt>\n",
        "Context (not visible in video, only for AI guidance):\n",
        "\n",
        "* You are creating the third and final part of an official intro video for Apple's new iPhone 19.\n",
        "* The previous 6-second scene ended clearly showing the back of the iPhone 19, focusing specifically on its advanced triple-lens camera module.\n",
        "\n",
        "Prompt: Final shot begins exactly from the final frame of the previous scene, clearly displaying the back side of the iPhone 19, with special emphasis on the triple-lens camera module. Now, have a user's hand gently pick up the phone, naturally rotating it from the back to the front and bringing it upward toward their face. Clearly show the phone smoothly and quickly unlocking via Face ID recognition, transitioning immediately to a vibrant home screen filled with updated app icons. Finish the scene by subtly fading the home screen into the iconic Apple logo. Keep the visual style consistent, premium, and elegant, suitable for an official Apple product launch.\n",
        "</prompt>\n",
        "\n",
        "--\n",
        "\n",
        "Notice how we broke up the initial prompt into multiple prompts that provide context and continuity so this all works seamlessly.\n",
        "\"\"\".strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8KOZMx1Ts6d"
      },
      "source": [
        "# 4) Planner: ask the LLM to generate prompts (JSON)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vtc8oHzSJiG"
      },
      "outputs": [],
      "source": [
        "def plan_prompts_with_ai(base_prompt: str, seconds_per_segment: int, num_generations: int):\n",
        "    \"\"\"\n",
        "    Calls OpenRouter API (via OpenAI SDK) to produce a JSON object:\n",
        "    {\n",
        "      \"segments\": [\n",
        "        {\"title\": \"...\", \"seconds\": <int>, \"prompt\": \"<full prompt block>\"},\n",
        "        ...\n",
        "      ]\n",
        "    }\n",
        "    Uses GPT-5 with reasoning enabled at medium effort level.\n",
        "    \"\"\"\n",
        "    # Compose a single plain-text input with the variables:\n",
        "    user_input = f\"\"\"\n",
        "BASE PROMPT: {base_prompt}\n",
        "\n",
        "GENERATION LENGTH (seconds): {seconds_per_segment}\n",
        "TOTAL GENERATIONS: {num_generations}\n",
        "\n",
        "Return exactly {num_generations} segments.\n",
        "\"\"\".strip()\n",
        "\n",
        "    # Use OpenRouter via chat completions with reasoning enabled\n",
        "    response = client.chat.completions.create(\n",
        "        model=PLANNER_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": PLANNER_SYSTEM_INSTRUCTIONS},\n",
        "            {\"role\": \"user\", \"content\": user_input}\n",
        "        ],\n",
        "        max_tokens=4000,  # tokens for the final answer\n",
        "        temperature=0.7,\n",
        "        extra_body={\n",
        "            \"reasoning\": {\n",
        "                \"effort\": \"medium\",  # \"low\" | \"medium\" | \"high\"\n",
        "            }\n",
        "        },\n",
        "    )\n",
        "\n",
        "    text = response.choices[0].message.content\n",
        "\n",
        "    # Extract the first JSON object found in the response text.\n",
        "    m = re.search(r'\\{[\\s\\S]*\\}', text)\n",
        "    if not m:\n",
        "        raise ValueError(\"Planner did not return JSON. Inspect response and adjust instructions.\")\n",
        "    data = json.loads(m.group(0))\n",
        "\n",
        "    # Basic validation and enforcement\n",
        "    segments = data.get(\"segments\", [])\n",
        "    \n",
        "    # Fail fast if we don't have enough segments\n",
        "    if len(segments) < num_generations:\n",
        "        raise ValueError(\n",
        "            f\"Planner returned {len(segments)} segments but {num_generations} were requested. \"\n",
        "            f\"LLM output needs fixing. Response: {text[:500]}\"\n",
        "        )\n",
        "    \n",
        "    # Truncate if we got more than requested\n",
        "    if len(segments) > num_generations:\n",
        "        segments = segments[:num_generations]\n",
        "\n",
        "    # Force durations to the requested number (some models might deviate)\n",
        "    for seg in segments:\n",
        "        seg[\"seconds\"] = int(seconds_per_segment)\n",
        "\n",
        "    return segments\n",
        "\n",
        "segments_plan = plan_prompts_with_ai(BASE_PROMPT, SECONDS_PER_SEGMENT, NUM_GENERATIONS)\n",
        "\n",
        "print(\"AI‑planned segments:\\n\")\n",
        "for i, seg in enumerate(segments_plan, start=1):\n",
        "    print(f\"[{i:02d}] {seg['seconds']}s — {seg.get('title','(untitled)')}\")\n",
        "    print(seg[\"prompt\"])\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWhkKTYDTp0c"
      },
      "source": [
        "# 5) Sora helpers (create → poll → download → extract final frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVZzK-vgSOMb"
      },
      "outputs": [],
      "source": [
        "import fal_client\n",
        "from pathlib import Path\n",
        "\n",
        "def create_and_poll_video(prompt: str, resolution: str, aspect_ratio: str, duration: int, input_image: Path | None = None):\n",
        "    \"\"\"\n",
        "    Create a video using Fal AI's Sora 2 Pro endpoint.\n",
        "    Automatically selects the appropriate endpoint:\n",
        "    - text-to-video if no input_image provided (first segment)\n",
        "    - image-to-video if input_image provided (subsequent segments for continuity)\n",
        "    \n",
        "    Returns the result dict from Fal AI.\n",
        "    \n",
        "    Args:\n",
        "        prompt: The text prompt for video generation\n",
        "        resolution: Video resolution - \"720p\" or \"1080p\"\n",
        "        aspect_ratio: Video aspect ratio - \"16:9\" or \"9:16\"\n",
        "        duration: Duration in seconds - 4, 8, or 12\n",
        "        input_image: Optional input image for image-to-video continuity\n",
        "    \"\"\"\n",
        "    \n",
        "    def on_queue_update(update):\n",
        "        if isinstance(update, fal_client.InProgress):\n",
        "            for log in update.logs:\n",
        "                if PRINT_PROGRESS_BAR:\n",
        "                    print(f\"  {log['message']}\")\n",
        "    \n",
        "    # Select endpoint based on whether we have an input image\n",
        "    if input_image is not None:\n",
        "        endpoint = SORA_IMAGE_TO_VIDEO\n",
        "        print(f\"  Using image-to-video endpoint (continuity mode)\")\n",
        "        print(f\"  Input image: {input_image.name}\")\n",
        "    else:\n",
        "        endpoint = SORA_TEXT_TO_VIDEO\n",
        "        print(f\"  Using text-to-video endpoint (first segment)\")\n",
        "    \n",
        "    # Build arguments for Fal API (matching official API documentation)\n",
        "    arguments = {\n",
        "        \"prompt\": prompt,\n",
        "        \"resolution\": resolution,\n",
        "        \"aspect_ratio\": aspect_ratio,\n",
        "        \"duration\": duration\n",
        "    }\n",
        "    \n",
        "    # If we have an input image, upload it and add to arguments\n",
        "    if input_image is not None:\n",
        "        image_url = fal_client.upload_file(str(input_image))\n",
        "        arguments[\"image_url\"] = image_url\n",
        "    \n",
        "    print(f\"  Submitting to Fal AI ({endpoint})...\")\n",
        "    print(f\"  Resolution: {resolution}, Aspect ratio: {aspect_ratio}, Duration: {duration}s\")\n",
        "    \n",
        "    # Subscribe and wait for completion\n",
        "    result = fal_client.subscribe(\n",
        "        endpoint,\n",
        "        arguments=arguments,\n",
        "        with_logs=True,\n",
        "        on_queue_update=on_queue_update,\n",
        "    )\n",
        "    \n",
        "    return result\n",
        "\n",
        "\n",
        "def download_fal_video(result: dict, out_path: Path) -> Path:\n",
        "    \"\"\"\n",
        "    Download the video from Fal AI result.\n",
        "    Fal AI returns a dict with 'video' key containing the URL.\n",
        "    \"\"\"\n",
        "    video_url = result.get(\"video\", {}).get(\"url\")\n",
        "    if not video_url:\n",
        "        raise RuntimeError(f\"No video URL in Fal AI result: {result}\")\n",
        "    \n",
        "    print(f\"  Downloading video from {video_url}...\")\n",
        "    \n",
        "    with requests.get(video_url, stream=True, timeout=600) as r:\n",
        "        r.raise_for_status()\n",
        "        with open(out_path, \"wb\") as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "    \n",
        "    return out_path\n",
        "\n",
        "\n",
        "def extract_last_frame(video_path: Path, out_image_path: Path) -> Path:\n",
        "    \"\"\"\n",
        "    Extract the last frame from a video file using OpenCV.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(f\"Failed to open {video_path}\")\n",
        "\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0\n",
        "    success, frame = False, None\n",
        "\n",
        "    if total > 0:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, total - 1)\n",
        "        success, frame = cap.read()\n",
        "    if not success or frame is None:\n",
        "        cap.release()\n",
        "        cap = cv2.VideoCapture(str(video_path))\n",
        "        while True:\n",
        "            ret, f = cap.read()\n",
        "            if not ret: break\n",
        "            frame = f\n",
        "            success = True\n",
        "    cap.release()\n",
        "\n",
        "    if not success or frame is None:\n",
        "        raise RuntimeError(f\"Could not read last frame from {video_path}\")\n",
        "\n",
        "    out_image_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    ok = cv2.imwrite(str(out_image_path), frame)\n",
        "    if not ok:\n",
        "        raise RuntimeError(f\"Failed to write {out_image_path}\")\n",
        "    return out_image_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_DfcHGPTmxJ"
      },
      "source": [
        "# 6) Chain generator (use planner output; continuity via final frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzAQxmwwTPhS"
      },
      "outputs": [],
      "source": [
        "def chain_generate_sora(segments, resolution: str, aspect_ratio: str, seconds_per_segment: int):\n",
        "    \"\"\"\n",
        "    segments: list of {\"title\": str, \"seconds\": int, \"prompt\": str}\n",
        "    resolution: Video resolution - \"720p\" or \"1080p\"\n",
        "    aspect_ratio: Video aspect ratio - \"16:9\" or \"9:16\"\n",
        "    seconds_per_segment: Duration per segment in seconds - 4, 8, or 12\n",
        "    Returns list of video segment Paths.\n",
        "    \n",
        "    For continuity:\n",
        "    - First segment uses text-to-video (no input image)\n",
        "    - Subsequent segments use image-to-video with the last frame from previous segment\n",
        "    \"\"\"\n",
        "    input_ref = None\n",
        "    segment_paths = []\n",
        "\n",
        "    for i, seg in enumerate(segments, start=1):\n",
        "        secs   = int(seg[\"seconds\"])\n",
        "        prompt = seg[\"prompt\"]\n",
        "\n",
        "        print(f\"\\n=== Generating Segment {i}/{len(segments)} — {secs}s ===\")\n",
        "        \n",
        "        # Generate video with Fal AI Sora 2 Pro\n",
        "        # First segment: input_ref is None (text-to-video)\n",
        "        # Subsequent segments: input_ref contains last frame (image-to-video)\n",
        "        result = create_and_poll_video(\n",
        "            prompt=prompt, \n",
        "            resolution=resolution, \n",
        "            aspect_ratio=aspect_ratio,\n",
        "            duration=seconds_per_segment,\n",
        "            input_image=input_ref\n",
        "        )\n",
        "        \n",
        "        # Download the video\n",
        "        seg_path = OUT_DIR / f\"segment_{i:02d}.mp4\"\n",
        "        download_fal_video(result, seg_path)\n",
        "        print(f\"  Saved {seg_path}\")\n",
        "        segment_paths.append(seg_path)\n",
        "\n",
        "        # Extract final frame for the next segment (if not the last segment)\n",
        "        if i < len(segments):\n",
        "            frame_path = OUT_DIR / f\"segment_{i:02d}_last.jpg\"\n",
        "            extract_last_frame(seg_path, frame_path)\n",
        "            print(f\"  Extracted last frame -> {frame_path}\")\n",
        "            input_ref = frame_path\n",
        "\n",
        "    return segment_paths\n",
        "\n",
        "\n",
        "def concatenate_segments(segment_paths, out_path: Path) -> Path:\n",
        "    \"\"\"\n",
        "    Concatenate video segments using MoviePy or ffmpeg fallback.\n",
        "    \"\"\"\n",
        "    if not segment_paths:\n",
        "        raise ValueError(\"No segments to concatenate\")\n",
        "    \n",
        "    if MOVIEPY_AVAILABLE:\n",
        "        # Use MoviePy\n",
        "        clips = [VideoFileClip(str(p)) for p in segment_paths]\n",
        "        target_fps = clips[0].fps or 24\n",
        "        result = concatenate_videoclips(clips, method=\"compose\")\n",
        "        result.write_videofile(\n",
        "            str(out_path),\n",
        "            codec=\"libx264\",\n",
        "            audio_codec=\"aac\",\n",
        "            fps=target_fps,\n",
        "            preset=\"medium\",\n",
        "            threads=0\n",
        "        )\n",
        "        for c in clips:\n",
        "            c.close()\n",
        "    else:\n",
        "        # Use ffmpeg directly\n",
        "        print(\"Using ffmpeg for concatenation...\")\n",
        "        \n",
        "        # Create a temporary file list for ffmpeg concat\n",
        "        list_file = OUT_DIR / \"concat_list.txt\"\n",
        "        with open(list_file, \"w\") as f:\n",
        "            for seg_path in segment_paths:\n",
        "                # ffmpeg concat demuxer requires absolute paths\n",
        "                f.write(f\"file '{seg_path.absolute()}'\\n\")\n",
        "        \n",
        "        # Run ffmpeg concat\n",
        "        import subprocess\n",
        "        cmd = [\n",
        "            FFMPEG_BIN,\n",
        "            \"-f\", \"concat\",\n",
        "            \"-safe\", \"0\",\n",
        "            \"-i\", str(list_file),\n",
        "            \"-c\", \"copy\",\n",
        "            str(out_path)\n",
        "        ]\n",
        "        \n",
        "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "        if result.returncode != 0:\n",
        "            raise RuntimeError(f\"ffmpeg concat failed: {result.stderr}\")\n",
        "        \n",
        "        # Clean up\n",
        "        list_file.unlink()\n",
        "    \n",
        "    return out_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G16vwi3MTj5e"
      },
      "source": [
        "# 7) Run the whole pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUcGUc_ITSA3"
      },
      "outputs": [],
      "source": [
        "# 1) (Already ran) Plan prompts with AI -> segments_plan\n",
        "# 2) Generate with Fal AI Sora 2 Pro in a chain\n",
        "segment_paths = chain_generate_sora(\n",
        "    segments_plan, \n",
        "    resolution=RESOLUTION, \n",
        "    aspect_ratio=ASPECT_RATIO,\n",
        "    seconds_per_segment=SECONDS_PER_SEGMENT\n",
        ")\n",
        "\n",
        "# 3) Concatenate\n",
        "final_path = OUT_DIR / \"combined.mp4\"\n",
        "concatenate_segments(segment_paths, final_path)\n",
        "print(\"\\nWrote combined video:\", final_path)\n",
        "\n",
        "# 4) Inline preview\n",
        "display(IPyVideo(str(final_path), embed=True, width=768))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOXbUOo9fJ0+9HX6bMZ/JfC",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
